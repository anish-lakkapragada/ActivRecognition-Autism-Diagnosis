{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Check whether mediapipe can detect the hands of any given video."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp \n",
    "import cv2 \n",
    "\n",
    "mp_hands = mp.solutions.hands \n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def show_hands(path): \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    with mp_hands.Hands(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                break \n",
    "\n",
    "            # Flip the image horizontally for a later selfie-view display, and convert\n",
    "            # the BGR image to RGB.\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "            # To improve performance, optionally mark the image as not writeable to\n",
    "            # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            results = hands.process(image)\n",
    "\n",
    "            # Draw the hand annotations on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            cv2.imshow('MediaPipe Hands', image)\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "    cv2.destroyAllWindows() \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_hands(\"/Users/anish/Documents/Machine Learning Env/AnishMachineLearning/control_far.mov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}