{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first step is preprocessing. We use the [SSBD Dataset](https://rolandgoecke.net/research/datasets/ssbd/) in order to get the data for hand flapping and spinning. \n",
    "\n",
    "## The dataset does contain 75 URLs (although we'll ignore all headbanging videos) to youtube videos. All the data is nicely stored in XML files that I will read to get the youtube videos and also the time stamps of when the behavior (hand flapping or spinning) occurs. Then I will use pytube to download the youtube videos to .mp4 and moviepy to cut the .mp4 videos into the areas of interest. Finally, because some of those areas of interest clips are more than a few seconds long (which is all you need to detect spinning or headbanging) I will take those areas that are > 8 seconds and split them into many clips (that way we have more data.) Also in sections of the video where no behavior is used I will take them as videos as control data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first change the directory over to ssbd release\n",
    "import os \n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step would be to get the hand flapping and spinning data \n",
    "\n",
    "tree = ET.parse(\"ssbd-release/Annotations/v_ArmFlapping_07.xml\")\n",
    "root = tree.getroot()\n",
    "for child in root:\n",
    "    # for each child in the root \n",
    "    if child.tag == \"url\":\n",
    "        print(child.text)\n",
    "    if child.tag == \"behaviours\":\n",
    "        for behavior in child: # go through each reported behavior \n",
    "            for tag in behavior: # tag is just the attribute of the behavior \n",
    "                if tag.tag == \"time\":\n",
    "                    print(tag.text)\n",
    "                if tag.tag == \"intensity\":\n",
    "                    print(tag.text)\n",
    "                if tag.tag == \"category\":\n",
    "                    print(tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_second(time : str) -> int:\n",
    "    # this will take in a time like \"0125\" or 1:25 and make it 85 (60 + 25)\n",
    "    overall_seconds = 0 \n",
    "    for i, time_char in enumerate(reversed(time)):\n",
    "        if i == 0:\n",
    "            overall_seconds += int(time_char)\n",
    "        if i == 1:\n",
    "            overall_seconds += int(time_char) * 10\n",
    "        if i == 2:\n",
    "            overall_seconds += int(time_char) * 60 \n",
    "        if i == 3:\n",
    "            overall_seconds += int(time_char) * 600 \n",
    "    return overall_seconds \n",
    "\n",
    "assert convert_to_second('2345') == 23 * 60 + 45  \n",
    "\n",
    "def consecutive(data, stepsize=1):\n",
    "    '''groups up elements in an array that are continous with each other (useful to create sections where none \n",
    "    of the behaviors are shown.)'''\n",
    "    return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math \n",
    "import numpy as np\n",
    "\n",
    "NUM_SECONDS_TO_RECOGNIZE = 8 # hypothesis: takes this many seconds seconds to recognize handflapping + spinning \n",
    "\n",
    "URLS_TO_DOWNLOAD = set() # contains all youtube videos to download \n",
    "links_to_times = {} # data will be stored here like {link : {'category' : [(start, end)], 'another cat' : [(start, end)]}\n",
    "\n",
    "\n",
    "for i, file_name in enumerate(os.listdir('ssbd-release/Annotations/')):\n",
    "    # parse this file \n",
    "    tree = ET.parse('ssbd-release/Annotations/' + file_name)\n",
    "    root = tree.getroot() \n",
    "    \n",
    "    # everything we need to store\n",
    "    URL = \"\"\n",
    "    \n",
    "    for child in root:\n",
    "        \n",
    "        if child.tag == \"url\":\n",
    "            URL = child.text # store URL\n",
    "            URLS_TO_DOWNLOAD.add(URL)\n",
    "            links_to_times[URL] = defaultdict(list) \n",
    "        \n",
    "        if child.tag == \"duration\":\n",
    "            duration = int(child.text[:-1])\n",
    "            all_times = list(range(duration))\n",
    "        \n",
    "        if child.tag == \"behaviours\": # this child has the list of behaviors \n",
    "            for reported_behavior in child: \n",
    "                for info in reported_behavior:\n",
    "                    # gather the start time, end time, and category for this youtube link \n",
    "                    if info.tag == \"time\":\n",
    "                        # the time will be start:end \n",
    "                        times = str(info.text) # contains the string \n",
    "                        if times.count(\":\"): \n",
    "                            divider_index = times.index(\":\")\n",
    "                        elif times.count(\"-\"):\n",
    "                            divider_index = times.index(\"-\")\n",
    "                        else:\n",
    "                            break # invalid then \n",
    "                        actual_start_time, actual_end_time = convert_to_second(times[:divider_index]), convert_to_second(times[divider_index + 1:])\n",
    "                        START_TIMES, END_TIMES = [], []\n",
    "                        times = np.array(range(actual_start_time, actual_end_time +1))\n",
    "                        for time in times:\n",
    "                            try:\n",
    "                                all_times.remove(time)\n",
    "                            except Exception as e:\n",
    "                                pass \n",
    "                        split_times = np.array_split(times, math.ceil(times.shape[0] / NUM_SECONDS_TO_RECOGNIZE))\n",
    "                        for time in split_times:\n",
    "                            START_TIMES.append(time[0])\n",
    "                            END_TIMES.append(time[-1])\n",
    "                    if info.tag == \"category\":\n",
    "                        # this is the label \n",
    "                        LABEL = info.text \n",
    "                \n",
    "                # create an entry for this reported behavior \n",
    "                for START_TIME, END_TIME in zip(START_TIMES, END_TIMES):\n",
    "                    links_to_times[URL][LABEL].append((START_TIME, END_TIME))\n",
    "                    \n",
    "            idle_times = consecutive(np.array(all_times))\n",
    "            num_contributed = 0 # each video can only give 4 control clips (because otherwise it takes WAY too long)\n",
    "            for control_times in idle_times:\n",
    "                if num_contributed >= 4: \n",
    "                    break \n",
    "                # times maybe > NUM_SECONDS_TO_RECOGNIZE so split if that is the case \n",
    "                if len(control_times) <= NUM_SECONDS_TO_RECOGNIZE:\n",
    "                    START_TIME, END_TIME = control_times[0], control_times[-1]\n",
    "                    links_to_times[URL]['control'].append((START_TIME, END_TIME))\n",
    "                    num_contributed += 1\n",
    "                else:\n",
    "                    # needs to be split \n",
    "                    control_times_split = np.array_split(control_times, math.ceil(len(control_times) / NUM_SECONDS_TO_RECOGNIZE))\n",
    "                    for control_time in control_times_split:  \n",
    "                        START_TIME, END_TIME = control_time[0], control_time[-1]\n",
    "                        links_to_times[URL]['control'].append((START_TIME, END_TIME))\n",
    "                        num_contributed += 1\n",
    "                        if num_contributed >= 4: \n",
    "                            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle this just incase \n",
    "import pickle \n",
    "with open(\"links_to_times.pkl\", 'wb') as f:\n",
    "    pickle.dump(links_to_times, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RARELY RUN THIS - DELETES ALL VIDEOS IN ALL BEHAVIOR_DATA DIRS\n",
    "\n",
    "for behavior in os.listdir(\"behavior_data\"): \n",
    "    if behavior == \".DS_Store\": continue \n",
    "    for file in os.listdir(\"behavior_data/\" + behavior): \n",
    "        os.remove(\"behavior_data/\" + behavior + \"/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import pytube\n",
    "FPS = 30 \n",
    "i = 0 \n",
    "for vid, (url, category_times) in enumerate((links_to_times.items())): \n",
    "    print(f\"staring the {vid+1}th file\")\n",
    "    # download the video \n",
    "    print(url)\n",
    "    try: \n",
    "        print(\"This is url: \", url)\n",
    "        y = pytube.YouTube(url)\n",
    "        video = y.streams.get_highest_resolution()\n",
    "        video.download()\n",
    "    except Exception as e:\n",
    "        print(f\"annoying url: {url}\")\n",
    "        print(e)\n",
    "        continue \n",
    "        \n",
    "    for category, times in category_times.items(): \n",
    "        folder_path = \"behavior_data/\" + category + \"/\"\n",
    "        \n",
    "        for start_time, end_time in times:\n",
    "            try:\n",
    "                input_file = y.streams.get_highest_resolution().default_filename\n",
    "                output_file = folder_path + f\"{i}.mp4\"\n",
    "                print(os.listdir(folder_path))\n",
    "                if f\"{i}.mp4\" not in os.listdir(folder_path):\n",
    "                    print(\"adding file\")\n",
    "                    with VideoFileClip(input_file) as video:\n",
    "                        new = video.subclip(start_time, end_time)\n",
    "                        new.write_videofile(output_file, audio_codec='aac')\n",
    "                        i += 1 \n",
    "            except Exception as e:\n",
    "                print(f\"failed on {i}\")\n",
    "                i += 1 \n",
    "    os.remove(y.streams.get_highest_resolution().default_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory Structure  \n",
    "\n",
    "## We have one folder inside of this AnishMachineLearning folder called \"behavior_data\" that has the \"armflapping\" and \"spinning\" folders. There all of the sliced .mp4 files with the behavior of interest are located. \n",
    "\n",
    "### We will process headbanging videos even if we are not going to use it because we still want it as a negative case for training the arm flapping & spinning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for spinning it is just the hand positions that matter right?\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp \n",
    "import numpy as np\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(\"/Users/anish/Documents/Machine Learning Env/AnishMachineLearning/behavior_data/spinning/37.mp4\")\n",
    "#cap = cv2.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "\n",
    "#capcv2.VideoCapture(0)\n",
    "\n",
    "hands = mp_hands.Hands(min_detection_confidence = 0.5, min_tracking_confidence = 0.5)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read() \n",
    "    if not ret:break \n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False \n",
    "    results = hands.process(image)\n",
    "\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    white_image = np.zeros_like(image)\n",
    "    white_image.fill(255.0)\n",
    "    \n",
    "    #check for hand results \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmark in results.multi_hand_landmarks:\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                x = int(landmark.x * width)\n",
    "                y = int(landmark.y * height)\n",
    "                cv2.circle(white_image, (x, y), 5, (100, 100, 0), -1)\n",
    "\n",
    "    cv2.imshow(\"\", white_image)\n",
    "\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break \n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can try using the y values for each of the hand flapping videos and graph them to see if there is a noticeable difference of the y-values (we'll use the mean of all y-values for all 21 hand landmarks and then graph them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "# first all hand flapping videos \n",
    "for hand_flap_video in os.listdir(\"behavior_data/armflapping\"):\n",
    "    video = \"behavior_data/armflapping/\" + hand_flap_video\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    hands = mp_hands.Hands(min_detection_confidence = 0.5, min_tracking_confidence = 0.5)\n",
    "    \n",
    "    all_YS = [] \n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read() \n",
    "        if not ret:break \n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False \n",
    "        results = hands.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        #check for hand results \n",
    "        y_s = []\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmark in results.multi_hand_landmarks:\n",
    "                for i in range(0, 21):\n",
    "                    landmark = hand_landmark.landmark[i]\n",
    "                    x = int(landmark.x * width)\n",
    "                    y = int(landmark.y * height)\n",
    "                    y_s.append(y)\n",
    "        \n",
    "        all_YS.append(np.mean(y_s))\n",
    "\n",
    "    plt.plot(range(len(all_YS)), all_YS, color = \"green\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for spinning \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "# first all hand flapping videos \n",
    "for hand_flap_video in os.listdir(\"behavior_data/spinning\"):\n",
    "    video = \"behavior_data/spinning/\" + hand_flap_video\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    hands = mp_hands.Hands(min_detection_confidence = 0.5, min_tracking_confidence = 0.5)\n",
    "    \n",
    "    all_YS = [] \n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read() \n",
    "        if not ret:break \n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False \n",
    "        results = hands.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        #check for hand results \n",
    "        y_s = []\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmark in results.multi_hand_landmarks:\n",
    "                for i in range(0, 21):\n",
    "                    landmark = hand_landmark.landmark[i]\n",
    "                    x = int(landmark.x * width)\n",
    "                    y = int(landmark.y * height)\n",
    "                    y_s.append(y)\n",
    "        \n",
    "        all_YS.append(np.mean(y_s))\n",
    "\n",
    "    plt.plot(range(len(all_YS)), all_YS, color = \"green\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next for headbanging \n",
    "\n",
    "# now for spinning \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "# first all hand flapping videos \n",
    "for hand_flap_video in os.listdir(\"behavior_data/headbanging\"):\n",
    "    video = \"behavior_data/headbanging/\" + hand_flap_video\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    hands = mp_hands.Hands(min_detection_confidence = 0.5, min_tracking_confidence = 0.5)\n",
    "    \n",
    "    all_YS = [] \n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read() \n",
    "        if not ret:break \n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False \n",
    "        results = hands.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        #check for hand results \n",
    "        y_s = []\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmark in results.multi_hand_landmarks:\n",
    "                for i in range(0, 21):\n",
    "                    landmark = hand_landmark.landmark[i]\n",
    "                    x = int(landmark.x * width)\n",
    "                    y = int(landmark.y * height)\n",
    "                    y_s.append(y)\n",
    "        \n",
    "        all_YS.append(np.mean(y_s))\n",
    "\n",
    "    plt.plot(range(len(all_YS)), all_YS, color = \"green\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First let's get the frames for every arm flapping and control video. If the number of frames is less than 100 frames we will not take it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "\n",
    "for video_name in os.listdir('behavior_data/armflapping'): \n",
    "    cap = cv2.VideoCapture('behavior_data/armflapping/' + video_name)  \n",
    "    FRAMES = [] # frames for this video \n",
    "    while True: \n",
    "        _, image = cap.read() \n",
    "        if not _ : break \n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        FRAMES.append(image)\n",
    "    if len(FRAMES) >= 100: \n",
    "        # ignore any .DS_Store files\n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "\n",
    "ARMFLAPPING_LABELS = np.ones(len(ARMFLAPPING_VIDEOS))\n",
    "\n",
    "for video_name in os.listdir('behavior_data/control'): \n",
    "    cap = cv2.VideoCapture('behavior_data/control/' + video_name)  \n",
    "    FRAMES = [] # frames for this video \n",
    "    while True: \n",
    "        _, image = cap.read() \n",
    "        if not _ : break \n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        FRAMES.append(image)\n",
    "        \n",
    "    if len(FRAMES) >= 100: \n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "\n",
    "CONTROL_LABELS = np.zeros(len(CONTROL_VIDEOS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and then balance the amount of videos\n",
    "amount_of_videos = min([len(CONTROL_VIDEOS), len(ARMFLAPPING_VIDEOS)])\n",
    "\n",
    "ARMFLAPPING_VIDEOS = np.array(ARMFLAPPING_VIDEOS)\n",
    "CONTROL_VIDEOS = np.array(CONTROL_VIDEOS)\n",
    "import numpy as np\n",
    "control_permutation = np.random.permutation(CONTROL_LABELS.shape[0])\n",
    "CONTROL_VIDEOS, CONTROL_LABELS = CONTROL_VIDEOS[control_permutation], CONTROL_LABELS[control_permutation]\n",
    "\n",
    "armflapping_permutation = np.random.permutation(ARMFLAPPING_LABELS.shape[0])\n",
    "ARMFLAPPING_VIDEOS, ARMFLAPPING_LABELS = ARMFLAPPING_VIDEOS[armflapping_permutation], ARMFLAPPING_LABELS[armflapping_permutation]\n",
    "\n",
    "ARMFLAPPING_VIDEOS, ARMFLAPPING_LABELS = ARMFLAPPING_VIDEOS[:amount_of_videos], ARMFLAPPING_LABELS[:amount_of_videos]\n",
    "CONTROL_VIDEOS, CONTROL_LABELS = CONTROL_VIDEOS[:amount_of_videos], CONTROL_LABELS[:amount_of_videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ARMFLAPPING_VIDEOS) == len(CONTROL_VIDEOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great we have gotten 75 videos of armflapping and 75 control videos. Every single video has a minimum of 100 frames. We have set the predetermined amount of frames that go into the LSTM at 100, because we don't want the model to overfit or care about the length of the video. For videos with more than 100 frames, we will only collect the first 100 frames. \n",
    "\n",
    "## When we get the x and y locations for where on the 21 hand landmarks, note that they will be based on the width / height of the video which varies from each video . One way to deal with this would be to simply make all frames the same width and height, however that may make it tough for mediapipe to actually find the landmarks. Because the average frame has roughly a height and width of 400x600 we will take whatever x and y values given for a frame and adjust them based on the frame's height/width divided by 400 /600. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "armflapping_videos_shapes = [] \n",
    "for video in ARMFLAPPING_VIDEOS: \n",
    "    armflapping_videos_shapes.append(list(np.array(video).shape)) # (frames, height, width, num channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(armflapping_videos_shapes, axis = 0) # average height and width for the armflapping frames is 403 x 562 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_videos_shapes = [] \n",
    "for video in CONTROL_VIDEOS:\n",
    "    control_videos_shapes.append(list(np.array(video).shape)) # (frames, height, width, num channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(control_videos_shapes, axis = 0) # average height and width for the armflapping frames is 419 x 581 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(35).reshape(5,7)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0::1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_armflapping_frames = [] # the dimensions of this will be (75, 100, height, width, channels)\n",
    "selected_control_frames = [] \n",
    "\n",
    "for FRAMES in ARMFLAPPING_VIDEOS: \n",
    "    selected_armflapping_frames.append(FRAMES[:100])\n",
    "\n",
    "for FRAMES in CONTROL_VIDEOS: \n",
    "    selected_control_frames.append(FRAMES[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_armflapping_frames = np.array(selected_armflapping_frames)\n",
    "selected_control_frames = np.array(selected_control_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final data should be (75, 100, 42)\n",
    "import matplotlib.pyplot as plt \n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence = 0.3, min_tracking_confidence = 0.3)\n",
    "\n",
    "for image in selected_armflapping_frames[22] :\n",
    "    height, width, _ = image.shape\n",
    "    image.flags.writeable = False \n",
    "    results = hands.process(image)\n",
    "    \n",
    "    y_s = []\n",
    "    x_s = []\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmark in results.multi_hand_landmarks:\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                x = int(landmark.x * width)\n",
    "                y = int(landmark.y * height)\n",
    "                y_s.append(y)\n",
    "                x_s.append(x)\n",
    "                \n",
    "                image = cv2.circle(image, (x, y), 5, (255, 0, 0), 2)\n",
    "    \n",
    "    print(y_s)\n",
    "    print(x_s)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "cap = cv2.VideoCapture('/Users/anish/Downloads/all_videos/armflapping/499.mp4')\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands: \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        print(ret)\n",
    "        # BGR 2 RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Flip on horizontal\n",
    "        image = cv2.flip(image, 1)\n",
    "        \n",
    "        # Set flag\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Detections\n",
    "        results = hands.process(image)\n",
    "        \n",
    "        # Set flag to true\n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        # RGB 2 BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Detections\n",
    "        print(results)\n",
    "        \n",
    "        # Rendering results\n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS, \n",
    "                                        mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                        mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2),\n",
    "                                         )\n",
    "            \n",
    "        # Save our image    \n",
    "        cv2.imwrite(os.path.join('Output Images', '{}.jpg'.format(uuid.uuid1())), image)\n",
    "        cv2.imshow('Hand Tracking', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create our custom layer for image augmentations. We will be using a transformation of brightness, rotation, height and width, and maybe even shear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "\n",
    "class AugLayer(keras.Layers.Layer): \n",
    "    def __init__(self, height_shift_range = 0.3, width_shift_range = 0.3, brightness_range = 0.2, rotation_range = 35): \n",
    "        self.height_shift_range = height_shift_range\n",
    "        self.width_shift_range = width_shift_range\n",
    "        self.brightness_range = brightness_range\n",
    "        self.rotation_range = rotation_range\n",
    "    def call(self, X, training=True): \n",
    "        if training: \n",
    "            # do augmentations\n",
    "            height_shift = np.random.uniform(-self.height_shift_range, self.height_shift_range)\n",
    "            \n",
    "        else: \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"/Users/anish/Documents/CP + Programming Fun/web dev **practice**/wedding/assets/proposal/square_proposal.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8833247e1406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow\u001b[0;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m     return NumpyArrayIterator(\n\u001b[0m\u001b[1;32m    867\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     super(NumpyArrayIterator, self).__init__(\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_data_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/keras_preprocessing/image/numpy_array_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mx_misc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "datagen.flow([[img, img]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
