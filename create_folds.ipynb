{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# We will be creating the folds for one-point tracking, six-point tracking, and all-points tracking!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "armflapping_videos:  15%|█▍        | 16/108 [00:00<00:02, 40.15it/s]failed on .DS_Store\n",
      "armflapping_videos: 100%|██████████| 108/108 [00:04<00:00, 24.01it/s]\n",
      "control_videos:  19%|█▉        | 12/62 [00:02<00:13,  3.75it/s]failed on .DS_Store\n",
      "control_videos: 100%|██████████| 62/62 [00:06<00:00, 10.01it/s]\n",
      "100%|██████████| 97/97 [03:30<00:00,  2.17s/it]\n",
      "100%|██████████| 50/50 [01:43<00:00,  2.07s/it]\n",
      "<ipython-input-2-f1dfb5cfac36>:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
      "<ipython-input-2-f1dfb5cfac36>:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
      "50it [00:00, 13007.21it/s]\n",
      "50it [00:00, 7687.79it/s](50, 90, 126) (50, 90, 126)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first track all points\n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import mediapipe as mp\n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence) # MAKE SURE THIS IS ALL GOOD \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 42 # use 0 as default if the class is not there \n",
    "    Y_locations = [0] * 42 \n",
    "    Z_locations = [0] * 42\n",
    "    x = y = z = 0 \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[x] = landmark.x\n",
    "                Y_locations[y] = landmark.y\n",
    "                Z_locations[z] = landmark.z\n",
    "                x += 1; y += 1; z += 1; \n",
    "    locations = np.concatenate([X_locations, Y_locations, Z_locations])\n",
    "    hands.close()\n",
    "    return locations \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_armflapping'), desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_armflapping/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "        \n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_control'), desc = \"control_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "        \n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "    \n",
    "# get the locations of all of the videos \n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "for FRAMES in tqdm(ARMFLAPPING_VIDEOS) :\n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "for FRAMES in tqdm(CONTROL_VIDEOS):  \n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 126))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS): \n",
    "    ARMFLAPPING_LABELS = np.ones(ARMFLAPPING_LOCATIONS.shape[0])\n",
    "    CONTROL_LABELS = np.zeros(CONTROL_LOCATIONS.shape[0])\n",
    "    \n",
    "    # concatenate \n",
    "    data = np.concatenate([ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS])\n",
    "    labels = np.concatenate([ARMFLAPPING_LABELS, CONTROL_LABELS])\n",
    "    \n",
    "    return data, labels \n",
    "\n",
    "X, y = generate_data(padded_armflapping_locations, padded_control_locations)\n",
    "\n",
    "import pickle, numpy as np \n",
    "\n",
    "# shuffle\n",
    "N = np.random.permutation(X.shape[0])\n",
    "X, y = X[N], y[N]\n",
    "\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"all_points_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "armflapping_videos:  18%|█▊        | 19/108 [00:00<00:02, 32.53it/s]failed on .DS_Store\n",
      "armflapping_videos: 100%|██████████| 108/108 [00:04<00:00, 26.11it/s]\n",
      "control_videos:  26%|██▌       | 16/62 [00:00<00:02, 21.18it/s]failed on .DS_Store\n",
      "control_videos: 100%|██████████| 62/62 [00:02<00:00, 21.41it/s]\n",
      "100%|██████████| 97/97 [03:30<00:00,  2.17s/it]\n",
      "100%|██████████| 50/50 [01:44<00:00,  2.09s/it]\n",
      "<ipython-input-2-3df64a89e251>:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
      "<ipython-input-2-3df64a89e251>:106: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
      "50it [00:00, 14383.76it/s]\n",
      "50it [00:00, 11908.19it/s](50, 90, 36) (50, 90, 36)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# next, track six points\n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    \"\"\"Only give 6 landmarks\"\"\"\n",
    "\n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence) # MAKE SURE THIS IS ALL GOOD \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 12\n",
    "    Y_locations = [0] * 12\n",
    "    Z_locations = [0] * 12\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        x = y = z = 0 \n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                if i not in [0, 4, 8, 12, 16, 20]: continue \n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[x] = landmark.x\n",
    "                Y_locations[y] = landmark.y \n",
    "                Z_locations[z] = landmark.z\n",
    "                x += 1; y += 1; z +=1; \n",
    "            \n",
    "    hands.close()\n",
    "    return np.concatenate([X_locations, Y_locations, Z_locations]) \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_armflapping'), desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_armflapping/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "        \n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_control'), desc = \"control_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "        \n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "    \n",
    "# get the locations of all of the videos \n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "for FRAMES in tqdm(ARMFLAPPING_VIDEOS) :\n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "for FRAMES in tqdm(CONTROL_VIDEOS):  \n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 36))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS): \n",
    "    ARMFLAPPING_LABELS = np.ones(ARMFLAPPING_LOCATIONS.shape[0])\n",
    "    CONTROL_LABELS = np.zeros(CONTROL_LOCATIONS.shape[0])\n",
    "    \n",
    "    # concatenate \n",
    "    data = np.concatenate([ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS])\n",
    "    labels = np.concatenate([ARMFLAPPING_LABELS, CONTROL_LABELS])\n",
    "    \n",
    "    return data, labels \n",
    "\n",
    "X, y = generate_data(padded_armflapping_locations, padded_control_locations)\n",
    "\n",
    "import pickle, numpy as np \n",
    "\n",
    "# shuffle\n",
    "N = np.random.permutation(X.shape[0])\n",
    "X, y = X[N], y[N]\n",
    "\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"six_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "armflapping_videos:  19%|█▉        | 21/108 [00:00<00:02, 37.71it/s]failed on .DS_Store\n",
      "armflapping_videos: 100%|██████████| 108/108 [00:06<00:00, 16.53it/s]\n",
      "control_videos:  23%|██▎       | 14/62 [00:01<00:03, 12.34it/s]failed on .DS_Store\n",
      "control_videos: 100%|██████████| 62/62 [00:05<00:00, 11.03it/s]\n",
      "100%|██████████| 97/97 [03:31<00:00,  2.18s/it]\n",
      "100%|██████████| 50/50 [01:45<00:00,  2.12s/it]\n",
      "<ipython-input-3-b0c24f89b18e>:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
      "<ipython-input-3-b0c24f89b18e>:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
      "50it [00:00, 14277.02it/s]\n",
      "50it [00:00, 14189.12it/s](50, 90, 6) (50, 90, 6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# finally, only track one point\n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    \"\"\"Only give the 0th landmark\"\"\"\n",
    "\n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence)  \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 2\n",
    "    Y_locations = [0] * 2\n",
    "    Z_locations = [0] * 2\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[hand] = landmark.x\n",
    "                Y_locations[hand] = landmark.y\n",
    "                Z_locations[hand] = landmark.z\n",
    "                break # take only the first landmark\n",
    "            \n",
    "    hands.close()\n",
    "    return np.concatenate([X_locations, Y_locations, Z_locations]) \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_armflapping'), desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_armflapping/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "        \n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_control'), desc = \"control_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "        \n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "    \n",
    "# get the locations of all of the videos \n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "for FRAMES in tqdm(ARMFLAPPING_VIDEOS) :\n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "for FRAMES in tqdm(CONTROL_VIDEOS):  \n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 6))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS): \n",
    "    ARMFLAPPING_LABELS = np.ones(ARMFLAPPING_LOCATIONS.shape[0])\n",
    "    CONTROL_LABELS = np.zeros(CONTROL_LOCATIONS.shape[0])\n",
    "    \n",
    "    # concatenate \n",
    "    data = np.concatenate([ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS])\n",
    "    labels = np.concatenate([ARMFLAPPING_LABELS, CONTROL_LABELS])\n",
    "    \n",
    "    return data, labels \n",
    "\n",
    "X, y = generate_data(padded_armflapping_locations, padded_control_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "armflapping_videos:  19%|█▉        | 21/108 [00:00<00:02, 37.46it/s]failed on .DS_Store\n",
      "armflapping_videos: 100%|██████████| 108/108 [00:05<00:00, 20.40it/s]\n",
      "control_videos:  23%|██▎       | 14/62 [00:01<00:04, 10.59it/s]failed on .DS_Store\n",
      "control_videos: 100%|██████████| 62/62 [00:06<00:00,  9.26it/s]\n",
      "100%|██████████| 97/97 [03:38<00:00,  2.25s/it]\n",
      "100%|██████████| 50/50 [01:47<00:00,  2.15s/it]\n",
      "<ipython-input-58-891006575167>:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
      "<ipython-input-58-891006575167>:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
      "50it [00:00, 9304.13it/s]\n",
      "50it [00:00, 5097.97it/s](50, 90, 126) (50, 90, 126)\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd \\nnew_X = []\\nfor i in range(X.shape[0]): \\n    matrix_video = X[i]\\n    new_data = np.zeros((X.shape[1], 3))\\n    temp_df = pd.DataFrame(matrix_video)\\n    temp_df[temp_df == 0] = None \\n    new_data[:, 0] = np.array(temp_df[:, :42].mean(axis = 1).fillna(0))\\n    new_data[:, 0] = np.array(temp_df[:, 42:84].mean(axis = 1).fillna(0))\\n    new_data[:, 0] = np.array(temp_df[:, 84:126].mean(axis = 1).fillna(0))\\n    new_X.append(new_data)\\nX = np.array(new_X)\\n\\nassert X.shape == (100, 90, 3)\\n\\nsplits = [] # stores k (X_i, y_i) splits\\nX_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\\nfor X_split, y_split in zip(X_splits, y_splits): \\n    splits.append((X_split, y_split))\\n\\nfor i, split in enumerate(splits): \\n    with open(f\"mean_point_folds/split{i+1}\", \\'wb\\') as f: \\n        pickle.dump(split, f)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "# first track all points\n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import mediapipe as mp\n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence) # MAKE SURE THIS IS ALL GOOD \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 42 # use 0 as default if the class is not there \n",
    "    Y_locations = [0] * 42 \n",
    "    Z_locations = [0] * 42\n",
    "    x = y = z = 0 \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[x] = landmark.x\n",
    "                Y_locations[y] = landmark.y\n",
    "                Z_locations[z] = landmark.z\n",
    "                x += 1; y += 1; z += 1; \n",
    "    locations = np.concatenate([X_locations, Y_locations, Z_locations])\n",
    "    hands.close()\n",
    "    return locations \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_armflapping'), desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_armflapping/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "        \n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_control'), desc = \"control_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "        \n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "    \n",
    "# get the locations of all of the videos \n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "for FRAMES in tqdm(ARMFLAPPING_VIDEOS) :\n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "for FRAMES in tqdm(CONTROL_VIDEOS):  \n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 126))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS): \n",
    "    ARMFLAPPING_LABELS = np.ones(ARMFLAPPING_LOCATIONS.shape[0])\n",
    "    CONTROL_LABELS = np.zeros(CONTROL_LOCATIONS.shape[0])\n",
    "    \n",
    "    # concatenate \n",
    "    data = np.concatenate([ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS])\n",
    "    labels = np.concatenate([ARMFLAPPING_LABELS, CONTROL_LABELS])\n",
    "    \n",
    "    return data, labels \n",
    "\n",
    "X, y = generate_data(padded_armflapping_locations, padded_control_locations)\n",
    "\n",
    "import pickle, numpy as np \n",
    "\n",
    "# shuffle\n",
    "N = np.random.permutation(X.shape[0])\n",
    "X, y = X[N], y[N]\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd \n",
    "new_X = []\n",
    "for i in range(X.shape[0]): \n",
    "    matrix_video = X[i]\n",
    "    new_data = np.zeros((X.shape[1], 3))\n",
    "    temp_df = pd.DataFrame(matrix_video)\n",
    "    temp_df[temp_df == 0] = None \n",
    "    new_data[:, 0] = np.array(temp_df[:, :42].mean(axis = 1).fillna(0))\n",
    "    new_data[:, 0] = np.array(temp_df[:, 42:84].mean(axis = 1).fillna(0))\n",
    "    new_data[:, 0] = np.array(temp_df[:, 84:126].mean(axis = 1).fillna(0))\n",
    "    new_X.append(new_data)\n",
    "X = np.array(new_X)\n",
    "\n",
    "assert X.shape == (100, 90, 3)\n",
    "\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"mean_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-fd70ed881dc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmatrix_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnew_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix_video\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;34m'Cannot apply_along_axis when any iteration dimensions are 0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         ) from None\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;31m# build a buffer for storing evaluations of func1d.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-fd70ed881dc3>\u001b[0m in \u001b[0;36mmean_locs\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"take in a single frame, and return the mean location\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m126\u001b[0m \u001b[0;31m# make sure this is a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mX_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlocs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_locs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def mean_locs(frame): \n",
    "    \"\"\"take in a single frame, and return the mean location\"\"\"\n",
    "    assert len(frame) == 126 # make sure this is a vector\n",
    "    X_locs, Y_locs, Z_locs = frame[:42], frame[42:84], frame[84:]\n",
    "    return [locs[np.nonzero(locs)].mean() for locs in [X_locs, Y_locs, Z_locs]]\n",
    "\n",
    "new_X = [] \n",
    "for video in range(X.shape[0]): \n",
    "    matrix_video = X[video]\n",
    "    new_X.append(np.apply_along_axis(mean_locs, 1, matrix_video))\n",
    "X = np.array(new_X, copy = True) \n",
    "assert len(X.shape) == 3\n",
    "assert (X == np.array(new_X)).all()\n",
    "\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"mean_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp_X.pkl\", 'wb') as f: \n",
    "    pickle.dump(X, f)\n",
    "\n",
    "with open(\"temp_X.pkl\", 'rb') as f: \n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}