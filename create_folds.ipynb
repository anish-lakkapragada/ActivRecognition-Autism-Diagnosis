{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will be creating the folds for one-point tracking, six-point tracking, and all-points tracking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "armflapping_videos:  15%|█▍        | 16/108 [00:00<00:02, 40.15it/s]failed on .DS_Store\n",
      "armflapping_videos: 100%|██████████| 108/108 [00:04<00:00, 24.01it/s]\n",
      "control_videos:  19%|█▉        | 12/62 [00:02<00:13,  3.75it/s]failed on .DS_Store\n",
      "control_videos: 100%|██████████| 62/62 [00:06<00:00, 10.01it/s]\n",
      "100%|██████████| 97/97 [03:30<00:00,  2.17s/it]\n",
      "100%|██████████| 50/50 [01:43<00:00,  2.07s/it]\n",
      "<ipython-input-2-f1dfb5cfac36>:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
      "<ipython-input-2-f1dfb5cfac36>:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
      "50it [00:00, 13007.21it/s]\n",
      "50it [00:00, 7687.79it/s](50, 90, 126) (50, 90, 126)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first track all points\n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import mediapipe as mp\n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence) # MAKE SURE THIS IS ALL GOOD \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 42 # use 0 as default if the class is not there \n",
    "    Y_locations = [0] * 42 \n",
    "    Z_locations = [0] * 42\n",
    "    x = y = z = 0 \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[x] = landmark.x\n",
    "                Y_locations[y] = landmark.y\n",
    "                Z_locations[z] = landmark.z\n",
    "                x += 1; y += 1; z += 1; \n",
    "    locations = np.concatenate([X_locations, Y_locations, Z_locations])\n",
    "    hands.close()\n",
    "    return locations \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_armflapping'), desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/armflapping' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "        \n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(os.listdir('behavior_data/control'), desc = \"control_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "        \n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "    \n",
    "# get the locations of all of the videos \n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "for FRAMES in tqdm(ARMFLAPPING_VIDEOS) :\n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "for FRAMES in tqdm(CONTROL_VIDEOS):  \n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 126))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_rowP])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS): \n",
    "    ARMFLAPPING_LABELS = np.ones(ARMFLAPPING_LOCATIONS.shape[0])\n",
    "    CONTROL_LABELS = np.zeros(CONTROL_LOCATIONS.shape[0])\n",
    "    \n",
    "    # concatenate \n",
    "    data = np.concatenate([ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS])\n",
    "    labels = np.concatenate([ARMFLAPPING_LABELS, CONTROL_LABELS])\n",
    "    \n",
    "    return data, labels \n",
    "\n",
    "X, y = generate_data(padded_armflapping_locations, padded_control_locations)\n",
    "\n",
    "import pickle, numpy as np \n",
    "\n",
    "# shuffle\n",
    "N = np.random.permutation(X.shape[0])\n",
    "X, y = X[N], y[N]\n",
    "\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"all_points_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "armflapping_videos:  13%|█▎        | 14/108 [00:03<00:14,  6.39it/s]OpenCV: Couldn't read video stream from file \"behavior_data/armflapping/.DS_Store\"\n",
      "[ERROR:0] global /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-xxsyexfp/opencv/modules/videoio/src/cap.cpp (162) open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-xxsyexfp/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): behavior_data/armflapping/.DS_Store in function 'icvExtractPattern'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float division by zero\n",
      "failed on .DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "armflapping_videos: 100%|██████████| 108/108 [00:34<00:00,  3.09it/s]\n",
      "control_videos:  19%|█▉        | 12/62 [00:05<00:25,  1.98it/s]OpenCV: Couldn't read video stream from file \"behavior_data/control/.DS_Store\"\n",
      "[ERROR:0] global /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-xxsyexfp/opencv/modules/videoio/src/cap.cpp (162) open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-xxsyexfp/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): behavior_data/control/.DS_Store in function 'icvExtractPattern'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float division by zero\n",
      "failed on .DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "control_videos: 100%|██████████| 62/62 [00:23<00:00,  2.69it/s]\n",
      "0it [00:00, ?it/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "41it [01:23,  2.16s/it]"
     ]
    }
   ],
   "source": [
    "# next, track six points\n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(32) # set a random seed \n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    \"\"\"Only give 6 landmarks\"\"\"\n",
    "\n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence) # MAKE SURE THIS IS ALL GOOD \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 12\n",
    "    Y_locations = [0] * 12\n",
    "    Z_locations = [0] * 12\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        x = y = z = 0 \n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                if i not in [0, 4, 8, 12, 16, 20]: continue \n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[x] = landmark.x\n",
    "                Y_locations[y] = landmark.y \n",
    "                Z_locations[z] = landmark.z\n",
    "                x += 1; y += 1; z +=1; \n",
    "            \n",
    "    hands.close()\n",
    "    return np.concatenate([X_locations, Y_locations, Z_locations]) \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "\n",
    "# we want to store the names of everything \n",
    "ARMFLAPPING_FILE_NAMES = []\n",
    "CONTROL_FILE_NAMES = []\n",
    "for video_name in tqdm(os.listdir('behavior_data/armflapping'), desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        os.mkdir(\"behavior_data/armflapping/\" + video_name[1:])\n",
    "        cap = cv2.VideoCapture('behavior_data/armflapping/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        i = 0\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "\n",
    "            cv2.imwrite(\"behavior_data/armflapping/\" + video_name[1:] + \"/\" + str(i + 1) + \".jpg\", image)\n",
    "            i += 1 \n",
    "\n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "        ARMFLAPPING_FILE_NAMES.append(video_name)\n",
    "\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(os.listdir('behavior_data/control'), desc = \"control_videos\"): \n",
    "    try: \n",
    "        os.mkdir(\"behavior_data/control/\" + video_name[1:])\n",
    "        cap = cv2.VideoCapture('behavior_data/control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "        i = 0\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "\n",
    "            cv2.imwrite(\"behavior_data/control/\" + video_name[1:] + \"/\" + str(i + 1) + \".jpg\", image)\n",
    "            \n",
    "            i += 1\n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "        CONTROL_FILE_NAMES.append(video_name)\n",
    "    \n",
    "\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "\n",
    "for FRAMES, file_name in tqdm(zip(CONTROL_VIDEOS, CONTROL_FILE_NAMES)):  \n",
    "    locs = []\n",
    "    for i, frame in enumerate(FRAMES): \n",
    "        locs.append(hand_locations(np.array(frame)))\n",
    "\n",
    "        #FRAME = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        #cv2.imwrite(\"behavior_data/control/\" + file_name[1:] + \"/\" + str(i + 1) + \".jpg\", frame)\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "\n",
    "# get the locations of all of the videos \n",
    "\n",
    "for FRAMES, file_name in tqdm(zip(ARMFLAPPING_VIDEOS,ARMFLAPPING_FILE_NAMES)) :\n",
    "    locs = []\n",
    "    for i, frame in enumerate(FRAMES): \n",
    "        locs.append(hand_locations(np.array(frame)))\n",
    "\n",
    "        #FRAME = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        #cv2.imwrite(\"behavior_data/armflapping/\" + file_name[1:] + \"/\" + str(i + 1) + \".jpg\", frame)\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "ARMFLAPPING_FILE_NAMES = np.array(ARMFLAPPING_FILE_NAMES[:N])\n",
    "CONTROL_FILE_NAMES = np.array(CONTROL_FILE_NAMES[:N])\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 36))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS): \n",
    "    ARMFLAPPING_LABELS = np.ones(ARMFLAPPING_LOCATIONS.shape[0])\n",
    "    CONTROL_LABELS = np.zeros(CONTROL_LOCATIONS.shape[0])\n",
    "    \n",
    "    # concatenate \n",
    "    data = np.concatenate([ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS])\n",
    "    labels = np.concatenate([ARMFLAPPING_LABELS, CONTROL_LABELS])\n",
    "    \n",
    "    return data, labels \n",
    "\n",
    "X, y = generate_data(padded_armflapping_locations, padded_control_locations)\n",
    "\n",
    "\n",
    "# get all file names \n",
    "FILE_NAMES = np.concatenate([ARMFLAPPING_FILE_NAMES, CONTROL_FILE_NAMES])\n",
    "\n",
    "import pickle, numpy as np \n",
    "\n",
    "# shuffle\n",
    "N = np.random.permutation(X.shape[0])\n",
    "X, y = X[N], y[N]\n",
    "FILE_NAMES = FILE_NAMES[N]\n",
    "\n",
    "# shuffle again with a random seed of 65\n",
    "\n",
    "np.random.seed(65)\n",
    "N = np.random.permutation(X.shape[0])\n",
    "X, y = X[N], y[N]\n",
    "FILE_NAMES = FILE_NAMES[N]\n",
    "\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "FILE_NAME_SPLITS = np.array_split(FILE_NAMES, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "    print(\"file names: \" , FILE_NAME_SPLITS)\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"six_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'behavior_data/shorter_armflapping'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/37/gk9g67jd759dj9nfbp9jjsyw0000gp/T/ipykernel_78708/3277026299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mARMFLAPPING_FPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# store the FPS of all armflapping videos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mCONTROL_FPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# store the FPS of all control videos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mvideo_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'behavior_data/shorter_armflapping'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"armflapping_videos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'behavior_data/armflapping'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvideo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'behavior_data/shorter_armflapping'"
     ]
    }
   ],
   "source": [
    "# finally, only track one point\n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    \"\"\"Only give the 0th landmark\"\"\"\n",
    "\n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence)  \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 2\n",
    "    Y_locations = [0] * 2\n",
    "    Z_locations = [0] * 2\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[hand] = landmark.x\n",
    "                Y_locations[hand] = landmark.y\n",
    "                Z_locations[hand] = landmark.z\n",
    "                break # take only the first landmark\n",
    "            \n",
    "    hands.close()\n",
    "    return np.concatenate([X_locations, Y_locations, Z_locations]) \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_armflapping'), desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/armflapping' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "        \n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(os.listdir('behavior_data/control'), desc = \"control_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "        \n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "    \n",
    "# get the locations of all of the videos \n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "for FRAMES in tqdm(ARMFLAPPING_VIDEOS) :\n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "for FRAMES in tqdm(CONTROL_VIDEOS):  \n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 6))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS): \n",
    "    ARMFLAPPING_LABELS = np.ones(ARMFLAPPING_LOCATIONS.shape[0])\n",
    "    CONTROL_LABELS = np.zeros(CONTROL_LOCATIONS.shape[0])\n",
    "    \n",
    "    # concatenate \n",
    "    data = np.concatenate([ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS])\n",
    "    labels = np.concatenate([ARMFLAPPING_LABELS, CONTROL_LABELS])\n",
    "    \n",
    "    return data, labels \n",
    "\n",
    "X, y = generate_data(padded_armflapping_locations, padded_control_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "armflapping_videos:  15%|█▍        | 16/108 [00:01<00:05, 16.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed on .DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "armflapping_videos: 100%|██████████| 108/108 [00:07<00:00, 13.88it/s]\n",
      "control_videos:  19%|█▉        | 12/62 [00:01<00:06,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed on .DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "control_videos: 100%|██████████| 62/62 [00:04<00:00, 14.39it/s]\n",
      "100%|██████████| 97/97 [03:31<00:00,  2.18s/it]\n",
      "100%|██████████| 50/50 [01:47<00:00,  2.15s/it]\n",
      "<ipython-input-29-44542a151008>:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
      "<ipython-input-29-44542a151008>:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
      "50it [00:00, 10563.93it/s]\n",
      "50it [00:00, 6572.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 90, 126) (50, 90, 126)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd \\nnew_X = []\\nfor i in range(X.shape[0]): \\n    matrix_video = X[i]\\n    new_data = np.zeros((X.shape[1], 3))\\n    temp_df = pd.DataFrame(matrix_video)\\n    temp_df[temp_df == 0] = None \\n    new_data[:, 0] = np.array(temp_df[:, :42].mean(axis = 1).fillna(0))\\n    new_data[:, 0] = np.array(temp_df[:, 42:84].mean(axis = 1).fillna(0))\\n    new_data[:, 0] = np.array(temp_df[:, 84:126].mean(axis = 1).fillna(0))\\n    new_X.append(new_data)\\nX = np.array(new_X)\\n\\nassert X.shape == (100, 90, 3)\\n\\nsplits = [] # stores k (X_i, y_i) splits\\nX_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\\nfor X_split, y_split in zip(X_splits, y_splits): \\n    splits.append((X_split, y_split))\\n\\nfor i, split in enumerate(splits): \\n    with open(f\"mean_point_folds/split{i+1}\", \\'wb\\') as f: \\n        pickle.dump(split, f)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first track all points\n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import mediapipe as mp\n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence) # MAKE SURE THIS IS ALL GOOD \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 42 # use 0 as default if the class is not there \n",
    "    Y_locations = [0] * 42 \n",
    "    Z_locations = [0] * 42\n",
    "    x = y = z = 0 \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[x] = landmark.x\n",
    "                Y_locations[y] = landmark.y\n",
    "                Z_locations[z] = landmark.z\n",
    "                x += 1; y += 1; z += 1; \n",
    "    locations = np.concatenate([X_locations, Y_locations, Z_locations])\n",
    "    hands.close()\n",
    "    return locations \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "for video_name in tqdm(os.listdir('behavior_data/shorter_armflapping'), desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/armflapping' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "        \n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(os.listdir('behavior_data/control'), desc = \"control_videos\"): \n",
    "    try: \n",
    "        cap = cv2.VideoCapture('behavior_data/shorter_control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "        \n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "    except Exception as e: \n",
    "        print(f\"failed on {video_name}\")\n",
    "    \n",
    "# get the locations of all of the videos \n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "for FRAMES in tqdm(ARMFLAPPING_VIDEOS) :\n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "for FRAMES in tqdm(CONTROL_VIDEOS):  \n",
    "    locs = []\n",
    "    for frame in FRAMES: \n",
    "        locs.append(hand_locations(frame))\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 126))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS): \n",
    "    ARMFLAPPING_LABELS = np.ones(ARMFLAPPING_LOCATIONS.shape[0])\n",
    "    CONTROL_LABELS = np.zeros(CONTROL_LOCATIONS.shape[0])\n",
    "    \n",
    "    # concatenate \n",
    "    data = np.concatenate([ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS])\n",
    "    labels = np.concatenate([ARMFLAPPING_LABELS, CONTROL_LABELS])\n",
    "    \n",
    "    return data, labels \n",
    "\n",
    "X, y = generate_data(padded_armflapping_locations, padded_control_locations)\n",
    "\n",
    "import pickle, numpy as np \n",
    "\n",
    "# shuffle \n",
    "N = np.random.permutation(X.shape[0])\n",
    "X, y = X[N], y[N]\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd \n",
    "new_X = []\n",
    "for i in range(X.shape[0]): \n",
    "    matrix_video = X[i]\n",
    "    new_data = np.zeros((X.shape[1], 3))\n",
    "    temp_df = pd.DataFrame(matrix_video)\n",
    "    temp_df[temp_df == 0] = None \n",
    "    new_data[:, 0] = np.array(temp_df[:, :42].mean(axis = 1).fillna(0))\n",
    "    new_data[:, 0] = np.array(temp_df[:, 42:84].mean(axis = 1).fillna(0))\n",
    "    new_data[:, 0] = np.array(temp_df[:, 84:126].mean(axis = 1).fillna(0))\n",
    "    new_X.append(new_data)\n",
    "X = np.array(new_X)\n",
    "\n",
    "assert X.shape == (100, 90, 3)\n",
    "\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"mean_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-cf37e4d2f41e>:5: RuntimeWarning: Mean of empty slice.\n",
      "  return [locs[np.nonzero(locs)].mean() for locs in [X_locs_1, X_locs_2, Y_locs_1, Y_locs_2, Z_locs_1, Z_locs_2] ]\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "def mean_locs(frame): \n",
    "    \"\"\"take in a single frame, and return the mean location\"\"\"\n",
    "    assert len(frame) == 126, len(frame) # make sure this is a vector\n",
    "    X_locs_1, X_locs_2, Y_locs_1, Y_locs_2, Z_locs_1, Z_locs_2 = frame[:21], frame[21:42], frame[42:63], frame[63:84], frame[84:105], frame[105:]\n",
    "    return [locs[np.nonzero(locs)].mean() for locs in [X_locs_1, X_locs_2, Y_locs_1, Y_locs_2, Z_locs_1, Z_locs_2] ]\n",
    "\n",
    "new_X = [] \n",
    "for video in range(X.shape[0]): \n",
    "    matrix_video = X[video]\n",
    "    new_X.append(np.apply_along_axis(mean_locs, 1, matrix_video))\n",
    "    \n",
    "X = np.array(new_X, copy = True) \n",
    "assert len(X.shape) == 3\n",
    "\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"mean_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp_X.pkl\", 'wb') as f: \n",
    "    pickle.dump(X, f)\n",
    "\n",
    "with open(\"temp_X.pkl\", 'rb') as f: \n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.nan_to_num(X)\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"mean_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control\"):\n",
    "         if os.path.isfile(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control/\" + file):\n",
    "             continue\n",
    "         else:\n",
    "             #shutil.rmtree(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/armflapping/\" + file)\n",
    "             for vid_name in os.listdir(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control/\" + file):\n",
    "                 image = cv2.imread(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control/\" + file + \"/\" + vid_name)\n",
    "                 image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                 os.remove(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control/\" + file + \"/\" + vid_name)\n",
    "                 cv2.imwrite(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control/\" + file + \"/\" + vid_name, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
